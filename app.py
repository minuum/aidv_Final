__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
import streamlit as st
import os
from glob import glob
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain_openai import OpenAIEmbeddings
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import Chroma
from dotenv import load_dotenv
load_dotenv()
import time
import json
from langchain_core.documents import Document
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from chromadb import chromadb
import time
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm 
import sys
sys.path.append("")
from function import DataTransformer
from chatbot_class import Chatbot

#==================data loading and embedding==================
OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
# documents = []
# # Data loading and chopping
# json_directory = "./dataset/common_senses"
# common_senses = ["TL_ë°°ì›€ê³¼ í•™ë¬¸"]
# dt = DataTransformer(json_directory=json_directory, common_senses=common_senses)
# json_datas, total_time = dt.load_json_files()

# # documents : json to text
# documents = [{"text": json.dumps(item)} for item in json_datas]

# # text_splits
# def split_document(doc):
#     return text_splitter.split_text(doc["text"])

# text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
# split_docs = []
# start_time = time.time()
# with ThreadPoolExecutor() as executor:
#     for split in tqdm(executor.map(split_document, documents), total=len(documents), desc="Splitting documents"):
#         split_docs.extend(split)

# end_time = time.time()
# total_time = end_time - start_time
# print(f"Total time taken for splitting: {total_time:.2f} seconds")
 
# # embedding
# # Document ê°ì²´ë¡œ ë³€í™˜
# chunks = [Document(page_content=doc) for doc in split_docs]
# print("Chunks split Done.")
# # embeddingsì€ OpenAIì˜ ì„ë² ë”©ì„ ì‚¬ìš©
# # vectordbëŠ” chromadbì‚¬ìš©í•¨
# embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)
# vectordb = Chroma.from_documents(documents=chunks, embedding=embeddings)
# retriever =vectordb.as_retriever()
#================== ìºì‹œ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ë¡œë”©, ë¶„í• , ì„ë² ë”© ===================

@st.cache_resource(show_spinner=False)
def load_and_process_data():
    json_directory = "./dataset/common_senses"
    common_senses = ["TL_ë°°ì›€ê³¼ í•™ë¬¸"]
    dt = DataTransformer(json_directory=json_directory, common_senses=common_senses)
    json_datas, total_time = dt.load_json_files()

    # documents : json to text
    documents = [{"text": json.dumps(item)} for item in json_datas]

    # text_splits
    def split_document(doc):
        return text_splitter.split_text(doc["text"])

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    split_docs = []
    start_time = time.time()
    with ThreadPoolExecutor() as executor:
        for split in tqdm(executor.map(split_document, documents), total=len(documents), desc="Splitting documents"):
            split_docs.extend(split)

    end_time = time.time()
    total_time = end_time - start_time
    print(f"Total time taken for splitting: {total_time:.2f} seconds")

    # embedding
    # Document ê°ì²´ë¡œ ë³€í™˜
    chunks = [Document(page_content=doc) for doc in split_docs]
    print("Chunks split Done.")
    # embeddingsì€ OpenAIì˜ ì„ë² ë”©ì„ ì‚¬ìš©
    # vectordbëŠ” chromadbì‚¬ìš©í•¨
    embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)
    vectordb = Chroma.from_documents(documents=chunks, embedding=embeddings)
    return vectordb

vectordb = load_and_process_data()
retriever = vectordb.as_retriever()
#=============== ê´€ë ¨ í•¨ìˆ˜ë“¤ ====================
def stream_data(response):
    for word in response.split(" "):
        yield word + " "
        time.sleep(0.02)

def pdf_load(dir):
    input_docs = []
    # Load all PDF files using PyPDFLoader
    input_pdf_files = glob(os.path.join(dir, '*.pdf'))
    for pdf_file in input_pdf_files:
        loader = PyPDFLoader(pdf_file)
        pdf_documents = loader.load()
        input_docs.extend(pdf_documents)

#============ í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸ ===================
def prompt_load(file_path):
    file_content = ""
    with open(file_path, 'r', encoding='utf-8') as file:
        file_content = file.read()
    return file_content
        
def update_prompt(service):
    if service == "ì§€ì‹ê²€ìƒ‰":
        file_path = "prompt_common_senses.txt"
        return prompt_load(file_path)
    elif service == "í€´ì¦ˆ":
        file_path = "prompt_quiz.txt"
        return prompt_load(file_path)


#=============ë³€ìˆ˜ ì´ˆê¸°í™”====================
if "OPENAI_API" not in st.session_state:
    st.session_state["OPENAI_API"] = OPENAI_API_KEY
# ê¸°ë³¸ ëª¨ë¸ì„ ì„¤ì •í•©ë‹ˆë‹¤.
if "model" not in st.session_state:
    st.session_state["model"] = "gpt-4o"
# ì±„íŒ… ê¸°ë¡ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
    
if "service" not in st.session_state:
    st.session_state["service"] = "ì§€ì‹ê²€ìƒ‰"

if "quiz_stage" not in st.session_state:
    st.session_state.quiz_stage = 0

if "correct_answers" not in st.session_state:
    st.session_state.correct_answers = 0

if "retriever" not in st.session_state:    
    st.session_state["retriever"]=retriever
    print("Retriever Done.")

if "prompt" not in st.session_state:
    if st.session_state["service"] == "ì§€ì‹ê²€ìƒ‰":
        file_path = "prompt_common_senses.txt"
        st.session_state["prompt"] = prompt_load(file_path)
    elif st.session_state["service"] == "í€´ì¦ˆ":
        file_path = "prompt_quiz.txt"
        st.session_state["prompt"] = prompt_load(file_path)
    else:
        st.session_state["prompt"] = '''
        ì„œë¹„ìŠ¤ê°€ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
    '''    


# pdfë¥¼ ì‚¬ìš©í•´ì„œ pdf(ë…¼ë¬¸)ì„ ëª¨ë‘ ë¡œë“œ

if __name__ == '__main__':
    
    ################# ì„¤ì •ì„ ìœ„í•œ ì‚¬ì´ë“œë°”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ apií‚¤ë¥¼ ë°›ì•„ì•¼ ì‹¤í–‰ë©ë‹ˆë‹¤. ##########################################
    with st.sidebar:
        st.title("ì„¤ì •")
        st.session_state["OPENAI_API"] = st.text_input("Enter API Key", st.session_state["OPENAI_API"], type="password")
        # ëª¨ë¸ì„ ì„ íƒí•©ë‹ˆë‹¤.
        st.session_state["model"] = st.radio("ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.", ["gpt-4o", "gpt-3.5-turbo"])
        # ë¼ë””ì˜¤ ë²„íŠ¼ì„ ì‚¬ìš©í•˜ì—¬ ì„œë¹„ìŠ¤ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
        st.session_state["service"] = st.radio("ë‹µë³€ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.", ["ì§€ì‹ê²€ìƒ‰", "í€´ì¦ˆ"])
    # Chatbotì„ ìƒì„±í•©ë‹ˆë‹¤.
    chatbot = Chatbot(api_key=st.session_state["OPENAI_API"],
                       retriever=retriever,
                       sys_prompt=st.session_state["prompt"],
                       model_name=st.session_state["model"])
    if st.session_state["service"] == "ì§€ì‹ê²€ìƒ‰":
        st.title("ì§€ì‹ê²€ìƒ‰ ì±—ë´‡ ğŸ“š")       
    if st.session_state["service"] == "í€´ì¦ˆ":
        st.title("ğŸ§ ì§€ì‹,ìƒì‹ í€´ì¦ˆ ì±—ë´‡ ğŸ§")

    # Create a sidebar for API key and model selection
    with st.expander("ì‚¬ìš©ë²•", expanded=True):
        if st.session_state["service"] == "ì§€ì‹ê²€ìƒ‰":
            st.markdown("""
                    - ì‹œì‚¬ ìƒì‹ì„ ì•Œë ¤ì£¼ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.
                    - ë‹µë³€ ë‚´ìš©ì€ ai-hubì˜ ì§€ì‹ê²€ìƒ‰ ëŒ€í™” ë°ì´í„°ì…‹ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.
                    - ì‚¬ìš©ìì˜ ë‹µë³€ ë¿ë§Œ ì•„ë‹ˆë¼ ìœ ì‚¬í•œ ì£¼ì œë‚˜ ë‹¨ì–´, ì¤‘ìš”í•œ ë‹¨ì–´ë“¤ì— ëŒ€í•œ ë§í¬ê¹Œì§€ ì¡´ì¬í•©ë‹ˆë‹¤.
                    """)
        if st.session_state["service"] == "í€´ì¦ˆ":
            st.markdown("""
                    - ì‹œì‚¬ ìƒì‹ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ë‹µë³€ì— ë§ëŠ” í€´ì¦ˆë¥¼ ì œê³µí•´ì£¼ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.
                    - ë‹µë³€ ë‚´ìš©ì€ ai-hubì˜ ì§€ì‹ê²€ìƒ‰ ëŒ€í™” ë°ì´í„°ì…‹ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.
                    - ì²«ë²ˆì§¸ ì…ë ¥ì€ ë¬¸ì œì˜ ì£¼ì œì— ëŒ€í•´ì„œ, ë‘ë²ˆì§¸ ì…ë ¥ë¶€í„°ëŠ” ë¬¸ì œì˜ ì •ë‹µì„ ë§ì¶”ê²Œ ë©ë‹ˆë‹¤.
                    """)
    ############################################ ì‹¤ì œ ì±—ë´‡ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•œ Streamlit ì½”ë“œ ###################################################
    for content in st.session_state.chat_history:
        with st.chat_message(content["role"]):
            st.markdown(content['message']) 

    if st.session_state["service"] == "ì§€ì‹ê²€ìƒ‰":
        if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”."):
            with st.chat_message("user"):
                st.markdown(prompt)
            with st.chat_message("ai"):
                response = chatbot.generate(str(st.session_state.chat_history[-2:]) + f"\n\n{prompt}")
                st.write_stream(stream_data(response))
            st.session_state.chat_history.append({"role": "user", "message": prompt})
            st.session_state.chat_history.append({"role": "ai", "message": response})  
    
    if st.session_state["service"] == "í€´ì¦ˆ":
        if prompt := st.chat_input("ë¬¸ì œë¥¼ ë¨¼ì € ì…ë ¥í•˜ì„¸ìš”."):
            with st.chat_message("user"):
                st.markdown(prompt)

            if st.session_state.quiz_stage % 2 == 0:  # ë¬¸ì œ ì£¼ì œë¥¼ ì…ë ¥í•˜ëŠ” ë‹¨ê³„
                with st.chat_message("ai"):
                    question = chatbot.generate(f"ì£¼ì œ: {prompt}\në¬¸ì œë¥¼ ë§Œë“¤ì–´ ì£¼ì„¸ìš”.")
                    for word in stream_data(question):
                        st.markdown(word)
                    st.session_state.chat_history.append({"role": "user", "message": prompt})
                    st.session_state.chat_history.append({"role": "ai", "message": question})
                    st.session_state.quiz_stage += 1
                    st.session_state.current_question = question
            else:  # ì •ë‹µì„ ì…ë ¥í•˜ëŠ” ë‹¨ê³„
                with st.chat_message("ai"):
                    if prompt.lower() in st.session_state.current_question.lower():
                        st.session_state.correct_answers += 1
                        response = f"ì •ë‹µì…ë‹ˆë‹¤! í˜„ì¬ê¹Œì§€ ë§ì¶˜ ì •ë‹µ ê°œìˆ˜: {st.session_state.correct_answers}"
                    else:
                        response = f"í‹€ë ¸ìŠµë‹ˆë‹¤. í˜„ì¬ê¹Œì§€ ë§ì¶˜ ì •ë‹µ ê°œìˆ˜: {st.session_state.correct_answers}"
                    for word in stream_data(response):
                        st.markdown(word)
                    st.session_state.chat_history.append({"role": "user", "message": prompt})
                    st.session_state.chat_history.append({"role": "ai", "message": response})
                    st.session_state.quiz_stage += 1
